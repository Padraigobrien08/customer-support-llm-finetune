# Training Configuration

# Model Configuration
model:
  # HuggingFace model identifier (required)
  model_id: null  # e.g., "gpt2", "meta-llama/Llama-2-7b-chat-hf"

# Data Configuration
data:
  # Path to training dataset file (JSONL format)
  train_file: "data/processed/train_seed.jsonl"
  
  # Path to validation dataset file (optional)
  val_file: null
  
  # Maximum sequence length for tokenization
  max_seq_length: 512

# LoRA Configuration (PEFT)
lora:
  # LoRA rank (controls number of trainable parameters)
  r: 8
  
  # LoRA alpha (scaling factor)
  alpha: 16
  
  # LoRA dropout rate
  dropout: 0.1

# Training Hyperparameters
training:
  # Per-device batch size
  batch_size: 2
  
  # Gradient accumulation steps (effective batch size = batch_size * gradient_accumulation_steps)
  gradient_accumulation_steps: 4
  
  # Number of training epochs
  num_epochs: 3
  
  # Learning rate
  learning_rate: 2e-4
  
  # Save checkpoint every N steps
  save_steps: 50

# Output Configuration
output:
  # Directory for saving model checkpoints
  output_dir: "outputs/run_001"
  
  # Number of checkpoints to keep (oldest are deleted)
  save_total_limit: 3

# Framework Options
framework:
  # Use TRL SFTTrainer instead of standard Trainer (requires trl package)
  use_trl: false
