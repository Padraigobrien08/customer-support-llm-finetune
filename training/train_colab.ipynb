{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Customer Support LLM Fine-tuning - Google Colab\n",
        "\n",
        "This notebook trains a customer support LLM using QLoRA (4-bit quantization) on Google Colab with GPU support.\n",
        "\n",
        "**Model: Mistral-7B-Instruct**\n",
        "\n",
        "## Setup\n",
        "\n",
        "1. Upload your training data to Colab\n",
        "2. Run all cells\n",
        "3. Download the trained adapter when complete"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q torch transformers accelerate peft datasets bitsandbytes sentencepiece pydantic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Training Configuration\n",
            "============================================================\n",
            "Model: Qwen/Qwen2.5-7B-Instruct\n",
            "Batch size: 2\n",
            "Epochs: 5\n",
            "Output directory: /content/outputs/customer_support_adapter\n",
            "============================================================\n",
            "\n",
            "⚠ Next: Mount Google Drive and configure data file paths in the next cell\n"
          ]
        }
      ],
      "source": [
        "# Model configuration\n",
        "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"  # Mistral-7B-Instruct\n",
        "\n",
        "# Data paths (upload your files to Colab)\n",
        "TRAIN_FILE = \"/content/train.jsonl\"  # Upload your train.jsonl here\n",
        "VAL_FILE = \"/content/val.jsonl\"  # Upload your val.jsonl here (optional)\n",
        "\n",
        "# LoRA configuration\n",
        "LORA_R = 16\n",
        "LORA_ALPHA = 32\n",
        "LORA_DROPOUT = 0.05\n",
        "\n",
        "# Training hyperparameters\n",
        "BATCH_SIZE = 2  # Can be higher on GPU\n",
        "GRADIENT_ACCUMULATION_STEPS = 4\n",
        "NUM_EPOCHS = 5\n",
        "LEARNING_RATE = 1e-4\n",
        "MAX_SEQ_LENGTH = 1024\n",
        "SAVE_STEPS = 25\n",
        "\n",
        "# Output\n",
        "OUTPUT_DIR = \"/content/outputs/customer_support_adapter\"\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  Model: {MODEL_ID}\")\n",
        "print(f\"  Training file: {TRAIN_FILE}\")\n",
        "print(f\"  Output: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Upload Training Data\n",
        "\n",
        "Upload your `train.jsonl` and `val.jsonl` files using the file browser on the left, or use the code below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounting Google Drive...\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "mount failed",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1744398119.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Mount Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mounting Google Drive...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"✓ Google Drive mounted at /content/drive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Create upload directory\n",
        "os.makedirs(\"/content\", exist_ok=True)\n",
        "\n",
        "print(\"Please upload your training files:\")\n",
        "print(\"1. train.jsonl\")\n",
        "print(\"2. val.jsonl (optional)\")\n",
        "print(\"\\nUse the file browser on the left, or run:\")\n",
        "print(\"  files.upload()\")\n",
        "\n",
        "# Uncomment to use file uploader:\n",
        "# uploaded = files.upload()\n",
        "# for filename in uploaded.keys():\n",
        "#     print(f'Uploaded {filename}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load Model with QLoRA (4-bit Quantization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.5. Load Model and Tokenizer\n",
        "\n",
        "Mistral doesn't require special authentication - we can load it directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mistral doesn't require authentication - we can load directly\n",
        "print(\"✓ Ready to load Mistral model (no authentication needed)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "GPU: Tesla T4\n",
            "GPU Memory: 15.83 GB\n",
            "\n",
            "Loading tokenizer...\n",
            "Loading model with 4-bit quantization (QLoRA)...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af154f330f694c5a80d2b42f2576f959",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc351429be29435a8df488816f7ec51e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "855489c1b5b3426f9a6148ddcecc7443",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5e3cd0f8ce0148259fc17f6a3d0432ed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c04cd79c2674cb6ab5d77a029efd2fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2780eb22ccd44c858c7c3c10585e55a1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "31bc414116a849569803fbdfa57bc258",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Model loaded with 4-bit quantization\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "# Check GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "if device == \"cuda\":\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "# Configure 4-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "print(\"\\nLoading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"Loading model with 4-bit quantization (QLoRA)...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "# Prepare model for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "print(\"✓ Model loaded with 4-bit quantization\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LoRA target modules: ['q_proj', 'v_proj', 'k_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
            "Trainable params: 40,370,176 || All params: 4,393,342,464 || Trainable%: 0.9189%\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Auto-detect target modules for Qwen2.5\n",
        "target_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "\n",
        "# Try to auto-detect, fallback to common ones\n",
        "try:\n",
        "    # Check model architecture\n",
        "    if hasattr(model, \"config\") and hasattr(model.config, \"architectures\"):\n",
        "        arch = model.config.architectures[0] if model.config.architectures else \"\"\n",
        "        if \"Qwen\" in arch:\n",
        "            target_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "        elif \"Mistral\" in arch or \"Llama\" in arch:\n",
        "            target_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
        "except:\n",
        "    pass\n",
        "\n",
        "print(f\"LoRA target modules: {target_modules}\")\n",
        "\n",
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    target_modules=target_modules,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Apply LoRA to model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "def print_trainable_parameters(model):\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"Trainable params: {trainable_params:,} || \"\n",
        "        f\"All params: {all_param:,} || \"\n",
        "        f\"Trainable%: {100 * trainable_params / all_param:.4f}%\"\n",
        "    )\n",
        "\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Checking Data Files...\n",
            "============================================================\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "\n❌ ERROR: Training file not found: /content/data/splits/train.jsonl\nPlease upload train.jsonl using one of the methods in the previous cell.\nSee DATA_UPLOAD_GUIDE.md for detailed instructions.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3594504051.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     raise FileNotFoundError(\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;34mf\"\\n❌ ERROR: Training file not found: {TRAIN_FILE}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;34mf\"Please upload train.jsonl using one of the methods in the previous cell.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: \n❌ ERROR: Training file not found: /content/data/splits/train.jsonl\nPlease upload train.jsonl using one of the methods in the previous cell.\nSee DATA_UPLOAD_GUIDE.md for detailed instructions."
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "def format_messages(messages, system_prompt=None):\n",
        "    \"\"\"Format messages for training.\"\"\"\n",
        "    if tokenizer.chat_template:\n",
        "        chat_messages = []\n",
        "        if system_prompt:\n",
        "            chat_messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
        "        chat_messages.extend([{\"role\": msg[\"role\"], \"content\": msg[\"content\"]} for msg in messages])\n",
        "        return tokenizer.apply_chat_template(\n",
        "            chat_messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False\n",
        "        )\n",
        "    else:\n",
        "        # Fallback formatting\n",
        "        parts = []\n",
        "        if system_prompt:\n",
        "            parts.append(f\"System: {system_prompt}\")\n",
        "        for msg in messages:\n",
        "            parts.append(f\"{msg['role'].capitalize()}: {msg['content']}\")\n",
        "        return \"\\n\".join(parts)\n",
        "\n",
        "def tokenize_function(examples, tokenizer, max_length):\n",
        "    \"\"\"Tokenize examples for training.\"\"\"\n",
        "    texts = []\n",
        "    for i in range(len(examples[\"messages\"])):\n",
        "        messages = examples[\"messages\"][i]\n",
        "        system_prompt = examples.get(\"system_prompt\", [None] * len(examples[\"messages\"]))[i]\n",
        "        text = format_messages(messages, system_prompt)\n",
        "        texts.append(text)\n",
        "    \n",
        "    tokenized = tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=None\n",
        "    )\n",
        "    \n",
        "    # For causal LM, labels are the same as input_ids\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    \n",
        "    return tokenized\n",
        "\n",
        "# Load datasets\n",
        "print(\"Loading training dataset...\")\n",
        "train_dataset = load_dataset(\"json\", data_files=TRAIN_FILE, split=\"train\")\n",
        "print(f\"Loaded {len(train_dataset)} training examples\")\n",
        "\n",
        "if os.path.exists(VAL_FILE):\n",
        "    print(\"Loading validation dataset...\")\n",
        "    val_dataset = load_dataset(\"json\", data_files=VAL_FILE, split=\"train\")\n",
        "    print(f\"Loaded {len(val_dataset)} validation examples\")\n",
        "else:\n",
        "    print(\"No validation file found, skipping validation\")\n",
        "    val_dataset = None\n",
        "\n",
        "# Tokenize\n",
        "print(\"\\nTokenizing datasets...\")\n",
        "tokenized_train = train_dataset.map(\n",
        "    lambda examples: tokenize_function(examples, tokenizer, MAX_SEQ_LENGTH),\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names\n",
        ")\n",
        "\n",
        "if val_dataset:\n",
        "    tokenized_val = val_dataset.map(\n",
        "        lambda examples: tokenize_function(examples, tokenizer, MAX_SEQ_LENGTH),\n",
        "        batched=True,\n",
        "        remove_columns=val_dataset.column_names\n",
        "    )\n",
        "else:\n",
        "    tokenized_val = None\n",
        "\n",
        "print(\"✓ Datasets prepared\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "\n",
        "# Training arguments\n",
        "eval_steps = SAVE_STEPS if tokenized_val else None\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    fp16=True,  # Use FP16 on GPU\n",
        "    logging_steps=10,\n",
        "    save_steps=SAVE_STEPS,\n",
        "    save_total_limit=3,\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False,\n",
        "    eval_strategy=\"steps\" if tokenized_val else \"no\",\n",
        "    eval_steps=eval_steps,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    warmup_steps=50,\n",
        ")\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,  # Causal LM, not masked LM\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"✓ Trainer configured\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Starting training...\")\n",
        "print(f\"Total steps: ~{len(tokenized_train) // (BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS) * NUM_EPOCHS}\")\n",
        "print(f\"Estimated time: ~2-4 hours on T4 GPU\\n\")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n✓ Training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Save Final Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Saving final model to {OUTPUT_DIR}...\")\n",
        "trainer.save_model()\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "print(\"✓ Model saved\")\n",
        "print(f\"\\nAdapter saved to: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Saving final model to {OUTPUT_DIR}...\")\n",
        "trainer.save_model()\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "print(\"✓ Model saved\")\n",
        "print(f\"\\nAdapter saved to: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Download Adapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Create zip file\n",
        "zip_path = \"/content/customer_support_adapter.zip\"\n",
        "shutil.make_archive(\n",
        "    zip_path.replace(\".zip\", \"\"),\n",
        "    \"zip\",\n",
        "    OUTPUT_DIR\n",
        ")\n",
        "\n",
        "print(f\"Adapter zipped to: {zip_path}\")\n",
        "print(\"\\nDownloading adapter...\")\n",
        "files.download(zip_path)\n",
        "\n",
        "print(\"\\n✓ Download complete!\")\n",
        "print(\"\\nTo use this adapter:\")\n",
        "print(f\"1. Extract the zip file\")\n",
        "print(f\"2. Copy the adapter to: outputs/run_002_qwen7b/\")\n",
        "print(f\"3. Update inference server: export ADAPTER_DIR=outputs/run_002_qwen7b\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
